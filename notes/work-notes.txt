V property summary bude jednodušší, když vyjmenuju všechny properties, které by uživatele mohlo zajímat.
Uživatel pak může vybírat v tom seznamu.
Je to jednodušší než nechat uživatele klikat přímo do summary.

Když si uživatel zvolí, že chce použít nějaké property pro expanzi, tak jakou metodou to poslat na server? A na jaký endpoint?

Idea je, že na serveru si stavím tu další otázku uživatele, ale ještě ji necommitnu do konverzace. Když si uživatel zvolí propertu, tak se přidá do té další otázky. Když uživatel pošle samotnou zprávu, kterou jsme spolu sestavili, tak BE opravdu přidá tu otázku do konverzace.
- Jak zpracovat tu další otázku? Určitě ne stejným způsobem jako první otázka.
- Přidat si ty zvolené properties do podstruktury, kterou stavím na back endu.
- Nechat si přeložit tu podstrukturu do Sparqlu.
- Říct LLM, aby mi dalo nějaké rozumné další properties?
FE: POST question-preview + 1 property.
BE: Přidá property do substructure preview + řekne LLM, aby lexikalizoval.
FE: GET question-preview.
BE: Vrátí lexikalizaci.
FE: PUT question-preview + 1 property?
Request na question-preview by možná vždycky měl obsahovat všechny properties, které uživatel tam chce. Tím umožním i to, že když uživatel chce odebrat property, tak stačí poslat PUT request s příslušnými parametry a BE vytvoří správné preview?
- Takže správný HTTP verb bude PUT.
FE si musí držet seznam properties, které si uživatel vybral.

Back end si udržuje stav konverzace.
2 možnosti, jak uživatel může zadat svůj follow-up otázku.
- Ručně napíše.
- Vybírá si properties a pak potvrdí.
- To by trochu zjednodušilo zpracování na back endu, když přijde zpráva.
- Pokud uživatel potvrdil, na back endu můžu jen potvrdit question preview.
- Pokud uživatel zadal ručně otázku, možná budu muset zpracovat jako první dotaz.

Konkrétní data v databázi nemám, ale možná bych mohl použít LLM, abych mohl doplnit do dotazu to, co uživatel chce.
Například chce: Tournaments that have taken place this year so far in which there are participants *from Vietnam*.
- Já dokážu vyznačit to, že dotaz se bude týkat tournament a player a country.
- Tak teď nějakým způsobem říct LLM, kam by měl uživatel doplnit specifické hodnoty, které chce ve svém dotazu mít - např. Vietnam.

*20/04/2025*
Jak píšu tu mock konverzaci, tak se mi zdá, že pořád bude lepší mít jen seznam namapovaných slov. Potom ty související itemy zobrazit až když uživatel si rozklikne to slovo.
- A nebo to nechat jako seznam, ale vhodně odrazit.
- Když item patří třeba do tournaments, tak vylistovat hned pod tournaments a ještě odrazit.
- JAJA to zní dobře.
Nevýhoda tohoto přístupu je, že se musím zastavit po nějakém stupni zanoření. Když to zobrazím v separátním oknu, tak teoreticky uživatel může klikat a jít do jakékoliv hloubky.

Major části, které musím nastudovat a implementovat.
1. Co poslat do LLM a jak promptovat, abych získal výstup, který potřebuju.
	3.a) Musím si vytvořit nějaký Dataspecer package. Nebo použít již existující.
	3.b) Musím získat přístup na náš LLM server - napsat do chatu.
2. Co si vytáhnout z toho Dataspecer package pro zpracování.
	- Vyřeším, pokud udělám krok 1.
3. Jak stáhnout Dataspecer package v kódu.
	- Poradit se se Štěpánem.
4. Přidat databázi.
	4.a) Podívat se na Entity Framework guide.
	4.b) Rozběhnout Sqlite nebo Postgres.
5. Nastudovat, jak převést data specification substructure na Sparql query (pro implementaci Sparql translation service).
6. Implementovat front end.

26/04/2025
Když si exportuju specifikaci z Dataspeceru, tak tam je .owl.ttl soubor, což je RDF.

Když mám data jako RDF, tak to je vždycky node, šipka a další node ne? Takže můžu teoreticky reprezentovat tady ty 2 věci v kódu?
- Určitě můžu uložit typ class.
- Určitě můžu uložit typ relationship.
- Myslím si, že typ property můžu taky.
Když se zeptám na summary RDF, tak mi LLAMA popíše class, property a relationship. Takže tady ty 3 věci budou items.

Když řeknu "summarize class ...", tak to popíše to, co je napsaný v tom RDF. Já si pod "summarize" představuju spíše kousek textu, který je srozumitelný pro netechnického člověka.

Please summarize the class "barrier-free access" in a few sentences so that a non-technical person can understand it.
The "Barrier-Free Access" class refers to information about how easily people with disabilities can access and use a particular location, such as a building or tourist destination. It describes the
accessibility features of the location from the user's point of view. This class is used to provide details about the accessibility of a place, which can be helpful for people who need to plan their
visits accordingly.
- Toto už je trochu lepší, ale je dost možné, že si bude domýšlet. Ale to asi nevadí, protože bych chtěl, aby z kontextu té datové specifikace udělal to shrnutí.

Data specification:
- Určitě si uložím RDF string.
- Uložím si items. Ale pokud je datová specifikace velká, tak asi nemůžu uložit všechno ne?
- Ještě bych si mohl uložit další věci jako datové struktury atd. Prostě kontext kolem toho.
Data specification item:
- Uložím si typ (class, property, relationship).
- Summary (některé si už můžu předem vygenerovat).

*02/05/2025*
Pořád řeším, jak rozdělit zpracování endpointů.
- Mít jeden controller.
- Nebo mít různé controllery pro každé endpointy.
Do specifikace jsem napsal, že request handler deleguje práci do ostatních modulů.

Myslím si, že dává smysl, že Program.cs mapuje endpoint. Nějaký controller bude volat metody na conversation service a případně jiných servisech. Pak ten výsledek přetransformuje do vhodné HTTP formy.
- Např. na některé věci bude volat IConversationService. Ale pak vrátí jen 201_Created nebo něco takového.
- To nedává smysl vracet z IConversationService.

Co vše musím udělat, abych měl funkční back end?
- Zpracovat Dataspecer package (DSL do OWL a pak uložit jako string).
- Přidat Entity Framework.
- Implementovat komunikaci s Ollama.
- Přidat potřebné metody do interface servisů v core.

Mapuju endpointy a podle těch endpointů definuju metody, které musejí servisy podporovat.

Přemýšlím, co si uložit pod datovou specifikaci. Jestli všechny její třídy a property.
- To může teoreticky být hodně.
- Stejně vždycky dávám tu specifikaci jako RDF do LLM.
- Takže si jen uložím ten OWL.
Jako podstrukturu si uložím prostě seznam itemů.
- K tomu ještě třeba suggested message.

Napadlo mě, jestli reprezentace datové specifikace může mít vliv na výsledný query podle jazyku.
- Např. já chci na výstupu vydat Sparql, tak možná reprezentovat datovou specifikaci jako RDF bude lepší než reprezentovat jako něco jiného.
- Kdybych chtěl SQL query, tak možná RDF bude mít horší performance.

Kde jsem skončil:
- Přidal jsem metodu AddUserMessage.
- Musím přidat zpracování, které bere v úvahu RDF itemy.

Teď musím pracovat na tom, jak vytvořit vstupní OWL pro LLM.
- Nebo možná markdown bude lepší tvar.
- Každopádně musím zpracovat OWL.TTL a DSV.TTL a ještě odkazy v nich. Pak si ty informace zpracuju do nějakého tvaru.
Musím se podívat do dokumentace Dataspeceru, abych viděl popis RDF a aplikačních profilů atd.

V Class mi zatím chybí rdfs:label a rdfs:comment. To musím vyřešit tak, že to vytáhnu z reusesPropertyValue.
Pro získání věcí v reusesPropertyValue budu podporovat následující formáty dat:
- Turtle
- N-triples
- JSON-LD
- RDF/XML
- RDF/JSON
- Zeptám se na konzultaci, jaké další formáty bych měl podporovat.
Musím nějak získat URI, které vede přímo na to RDF a to můžu předat přímo do dotnetRDF pro načtení grafu.
slovník.gov.cz má ti URIčka úplně dole a ty odkazy vedou na dokumenty, které potřebuju. Ale je problém nějak vůbec získat to URI ... použít LLM?

Myslím si, že prozatím tam implementuju jen primitivní zpracování - prostě se pokusím otevřít ten odkaz v dsv:ReusedFromProperty a získat hodnotu. Pokud to neloadne RDF graf, tak prostě pokračuju bez toho.
Refaktoruju kód ve třídě DataSpecificationService, aby všechno nebylo v jedné metodě.

Otázky:
- Fasáda: dává smysl celou metodu pro konverzi dát do fasády nebo je lepší to zpracování mít v DataSpecificationService a detailní volání nějakých dotnetRdf metod dát do fasády?

## 22. 6. 2025

Minule jsem začal přesouvat komplikované zpracování s dotnetRDF do fasády. Není to ještě hotové, ale prozatím toho nechám.

Chtěl bych "zprovoznit" ten back end. Fasáda nemá úplně vliv na to, jestli back end bude něco vracet nebo zpracovávat, takže to nechám na později.

Dívám se na to, co posílal Štěpán a zkusím vymyslet, jak z toho získat DSV.

Dataspecer používá pro každý package nějaké UUID, kterému říká IRI a je to vidět v odkazech třeba na dokumentaci.

V tuto chvíli je mi teoreticky jedno, v jakém formátu Dataspecer vrací data. Jestli json-ld nebo dsv nebo owl. Já potřebuju něco, co nacpu do LLM, abych měl první funkční implementaci.
- Takže teď to nebudu podrobně zkoumat a implementuju prostě něco. Klidně třeba přímo to json-ld.
- Pak se uvidí.

Nejdřív to zkusím zprovoznit, protože musím se ještě napojit na ollama a vymyslet pár promptů. A pak zpracování těch promptů.

Získání json-ld dokumentaci prozatím funguje.
Teď musím implementovat ten workflow té konverzace.

Napadlo mě, že bych mohl zkusit použít HttpClient a přímo stáhnout dokumentaci package z Dataspeceru.
- Zkusil jsem to implementovat a vypadá to, že to funguje.

Napadlo mě, že nemusím ukládat stažený ZIP, ale stačí otevřít přímo z toho byte array.
Dále přidat interface PromptConstructor a ResponseProcessor do interface LlmConnector.

## 1. 7. 2025

Front end skeleton funguje.
Gemini ani ChatGPT nezobrazují timestamp. Pro moje účely to taky není důležité, takže to nejspíše zahodím.
Nesmažu, ale jen zakomentuju timestampy. Kdybych v budoucnu náhodou potřeboval.

## 13. 7. 2025

Proces vytvoření nové konverzace je rozdělený do dvou kroků:
1. POST data-specifications
2. POST conversations, kam předám IRI (nebo possibly ID) nově vytvořené specifikace v kroku 1.
Ale mohl bych to sjednotit.
- Když POST conversations dostane IRI, tak si vyhledá v databázi.
- Pokud tam není data specification, tak založí novou.
- Jinak použije stávající specifikaci pro novou konverzaci.

Uživatel zadá potřebné věci a klikne na vytvořit.
Front end zobrazí nějaký loading, třeba točící se kolečko.
Back end bude logovat každý krok pro případný debugging.