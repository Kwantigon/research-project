Štěpán Stenchlák:
Ahoj, nejsem si jistý, zda jsem správně pochopil, co vlastně s @Aleš Nguyen plánujete dělat s těmi JSONy a SPARQLy, tak jsem to chtěl zkusit zrekapitulovat trochu z jiného pohledu, ale teď zpětně nevím, jestli jsem to akorát nezkomplikoval.
Zjednodušeně lze na problém datových specifikací v Dataspeceru koukat pomocí tří vrstev: sémantické, strukturální a datové. Sémantickou vrstvou myslím popis konceptů, jejich vlastností a vztahů mezi nimi. Jedná se de-facto o konceptuální model, ale není přímo určen pro popis struktury dat. K tomu je strukturální vrstva, která dává koncepty ze sémantické vrstvy do konkrétního tvaru kterému pak můžou odpovídat konkrétní data na datové vrstvě.
Aktuálně umíme pracovat s JSONem, CSV, XML a RDFkem. Abychom popsali strukturu/tvar dat v jednom z těchto formátů, potřebujeme strukturální model. Pro JSON, CSV a XML máme Dataspecer strukturální editor, kde si ji uživatel může libovolně nadefinovat. Všechny tři technologie sdílejí jeden typ modelu kterému zatím říkáme "PSM". U RDFka je to trochu složitější, protože jsme zatím vždy předpokládali, že naše sémantika je implicitně mapovatelná na RDF (protože by i měla být), takže žádný strukturální model není potřeba. Nicméně formálně si ho tam můžeme představit jako nějaký "triviální strukturální model pro RDF".
Data pak odpovídají vždy nějakému strukturálnímu modelu ze kterého generujeme konkrétní schémata pro validaci dat, tedy JSON Schema, CSVW, XSD, RDFS, SHACL, SHEX.
Také umíme generovat skripty/mapování, které umí převádět data z jedné struktury na druhou. Umíme JSON <=> RDF, XML <=> RDF, CSV <=> RDF. Pokud tedy mám v CSV tabulku s osobami (jméno, příjmení, datum narození, ...), můžu ji pomocí CSVW převést na RDF a pomocí JSON-LD převést do JSONu.
Ta schémata nemusí obsahovat stejnou množinu konceptů. Pokud by například ve strukturálním modelu pro ten JSON chybělo datum narození, tak se mi prostě vygeneruje JSON bez data narození. Stejně tak umíme převádět v rámci jednoho formátu. Můžeme mít komplexní strukturální model a jednoduchý strukturální model a pokud máme data v JSONu která odpovídají tomu komplexnímu (tedy odpovídají JSON Schématu které bylo vygenerováno z komplexního strukturálního modelu), dokážeme převodem do RDF a zpět vygenerovat JSON data která odpovídají tomu jednoduchému modelu.
Teď k dotazování. Dotazování chápu jako proces, kdy vezmu data dle schématu A a vyplivnu data ve schématu B, ale navíc ta data mohou být nějak filtrována (pomocí WHERE podmínky) a nějak agregována.
Pokud mám vstupní data v nějakém formátu a struktuře, měl bych si tedy v Dataspeceru udělat strukturální model tak, aby ta data tomu modelu odpovídala. (U RDF nemusím nic dělat, protože ta struktura tam už je implicitně) Pokud pak budu chtít udělat dotaz typu SELECT name, surname FROM person nad JSON daty tak, aby výsledek byl v jiném JSONu, tak to vlastně znamená vytvořit strukturální model pro vstupní data a nový strukturální model který bude mít jen name a surname a pak si nechat vygenerovat ty transformace přes RDF pomocí JSON-LD. Pokud mám data v RDFku, konkrétně ve SPARQL endpointu a chci opět dostat JSON odpovídající dotazu výše, tak si jen nechám vygenerovat navíc SPARQL query a pak data převedu pomocí JSON-LD.
Co vůbec neumíme jsou WHERE podmínky a agregace. WHERE by se dal chápat jako nějaké omezení v tom strukturálním modelu (to jste myslím říkali, takže snad to chápu dobře), kdy řeknu, že nějaký atribut musí být třeba true. Agregaci teď fakt netuším, ale řekněme, že by se to taky dalo nějak do toho strukturálního modelu vecpat.
Pak by se vlastně jednalo o to, čím nahradit ty artefakty JSON-LD, XSD, CSVW a další, aby to umělo i tyto transformace dat (agregace a filtrování) nebo udělat zkratky a generovat třeba XML přímo z JSONu (ne že by na to byl use-case).
Co mi z toho tedy vyplývá je to, že vytvořit SPARQL dotaz je vlastně: (i) vytvoření "netriviálního strukturálního modelu pro RDF" který v sobě nese informace o tom SELECTU, WHERE podmínkách a případně agregacích a (ii) vygenerování finálního SPARQLu z tohoto modelu, který nám umožní převádět data z RDF do RDF v případě SPARQL CONSTRUCT. A v případě SPARQL SELECT se jedná o vytvoření strukturálního modelu pro CSV s těmi informacemi o WHERE podmínkách a agregacích a opět generování vhodného artefaktu.
Neříkám, že to tak musí být naimplementováno, jen že se to dá takto myšlenkově zasadit. A mimochodem, něco jako netriviální strukturální model pro RDF máme, když vlastně z PSMka dokážeme generovat SHACL a SHEX.

Martin Nečaský:
Díky za summary, rekapituluješ to naprosto přesně. Takhle přesně jsem to myslel a ani jsem to nedomýšlel až k agregačním dotazům. V podstatě tedy uživatel bere jako vstup datovou specifikaci a svojí konverzací s naším botem vytváří novou specifikaci - ta k těm stávajícím strukturálním modelům přidává nové "netriviální strukturální modely pro RDF". Nevím, jak moc netriviální má být ten "netriviální strukturální model pro RDF". K tomu jsem pokládal otázku, jak má vypadat. Odpověď jsme si neřekli, jen nějaké možnosti, tak to malinko rozvinu. Netriviální strukturální model pro RDF =
1. Jen ten výsledný SPARQL dotaz. SPARQL můžeme zapsat strojověji, existuje i zápis SPARQL dotazu v RDF triplech. Netřeba nic vymýšlet. Vazbu na sémantickou vrstvu držíme tím, co je ve SPARQL dotazu použito za pojmy (properties, classes).
2. Výsledný SPARQL dotaz s nějakými anotacemi popisujícímí vazby na sémantickou vrstvu, které nejdou vyjádřit prostým SPARQLem. Zatím mě žádné nenapadají, ale pořádně jsem se nazamýšlel.
3. Něco úplně nového, z čeho ten SPARQL budeme generovat.
Mně připadá, že bychom mohli jít u Alešova projektu cestou 1. Zdá se mi ale, že nemůžeme vytvořený SPARQL dotaz chápat jako plnohodnotný strukturální model ne? 1) nevím, co je kořenem, 2) chybí v něm různé další technické detaily, které z konceptuálního modelu dělají strukturální, jak popisuješ. Problém odvození strukturálního modelu ze SPARQLu, pokud to vůbec dává smysl, bych nechal na jiný projekt. U Aleše by stačilo, kdybychom do datových specifikací umožnili přidávat další typ modelů - dotazy ve SPARQLu.

Petr Škoda:
Z toho co jsi napsal mě jen napadá, jestli strukturální model musí vždy mít kořen? Tedy jestli je to vlastně nějaká selekce z konceptuálního modelu + pak mapování na strom?

Martin Nečaský:
Zatím  jsme to tak stavěli, že strukturální model má nějakou hlavní entitu a ona vždy vycházela jako kořen. Tj. tvojí otázku bych ještě zdetailnil: Musí mít strukturální model vždy nějakou hlavní entitu? Pokud ano, musí být kořenem?

Štěpán Stenchlák:
No otázkou je, co je to strukturální model.
V případě "našeho PSM" tak jak ho máme, tak tam je možnost mít více kořenů - například pokud pracuješ s SQL schématem, tak bys mohl chtít rovnou navrhovat dané tabulky. Nicméně v současnosti to všechny generátory ignorují a UI s tím neumí pracovat. Nemít žádný kořen mi také dává smysl, například pro nějakou grafovou databázi, kde chci specifikovat nějaký graf. Tam by nemělo být potřeba vybírat kořen.
Tímto se dostávám k té původní otázce, co je to strukturální model. No a podle mě můžou existovat nějaké další formáty pro jiné technologie, které také můžeme klasifikovat jako strukturální modely. Kupříkladu v současném PSM uvažujeme nad asociacemi jen v jednom směru, ale pro nějakou grafovou databázi chceme zachovat oba směry, takže budeme muset použít jiné prvky.
No a to podle mě ta 1) a 2) od Martina jsou strukturální modely. SPARQL CONSTRUCT (klidně zapsaný jako text - takže je to vlastně model :smirk:) se dá chápat (konkrétně můžeme jej interpretovat) jako strukturální model, protože ti určuje strukturu a mapování do sémantiky a navíc díky němu dostaneme i to filtrování a agregace. Obdobně SPARQL SELECT je teoreticky strukturální model pro popis CSVček.
Naopak, naše současné PSMko je vlastně takový jiný SELECT pro hierarchické výstupy - umožní nám přejmenovat položky, nastavit jim pořadí, vybrat které nás zajímají a vlastně i trochu měnit jejich strukturu.

Martin Nečaský:
Ano, souhlas, dává to smysl.
